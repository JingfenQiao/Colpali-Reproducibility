{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colpali_engine.models.late_interaction.colclip_architecture import ColClip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SiglipModel\n",
    "model = SiglipModel.from_pretrained(\"google/siglip-large-patch16-256\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SiglipModel(\n",
       "  (text_model): SiglipTextTransformer(\n",
       "    (embeddings): SiglipTextEmbeddings(\n",
       "      (token_embedding): Embedding(32000, 1024)\n",
       "      (position_embedding): Embedding(64, 1024)\n",
       "    )\n",
       "    (encoder): SiglipEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x SiglipEncoderLayer(\n",
       "          (self_attn): SiglipSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): SiglipMLP(\n",
       "            (activation_fn): PytorchGELUTanh()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "    (head): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  )\n",
       "  (vision_model): SiglipVisionTransformer(\n",
       "    (embeddings): SiglipVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16), padding=valid)\n",
       "      (position_embedding): Embedding(256, 1024)\n",
       "    )\n",
       "    (encoder): SiglipEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x SiglipEncoderLayer(\n",
       "          (self_attn): SiglipSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): SiglipMLP(\n",
       "            (activation_fn): PytorchGELUTanh()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "    (head): SiglipMultiheadAttentionPoolingHead(\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): SiglipMLP(\n",
       "        (activation_fn): PytorchGELUTanh()\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SiglipProcessor\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "processor = SiglipProcessor.from_pretrained(\"google/siglip-large-patch16-256\")\n",
    "\n",
    "url = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "inputs = processor(text = [\"Hello how are you?\"], images=image, return_tensors=\"pt\", padding=True).to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[14647,   364,   280,   273,     1]], device='cuda:0'), 'pixel_values': tensor([[[[ 0.4431,  0.4667,  0.4824,  ...,  0.9137,  0.9059,  0.9059],\n",
       "          [ 0.4588,  0.4745,  0.4980,  ...,  0.9137,  0.9059,  0.9059],\n",
       "          [ 0.4745,  0.4980,  0.5216,  ...,  0.9137,  0.9137,  0.9059],\n",
       "          ...,\n",
       "          [-0.4667, -0.4431, -0.3961,  ..., -0.2000, -0.2235, -0.2078],\n",
       "          [-0.4588, -0.4353, -0.4275,  ..., -0.2000, -0.2000, -0.2235],\n",
       "          [-0.4431, -0.4510, -0.4510,  ..., -0.1765, -0.2392, -0.2863]],\n",
       "\n",
       "         [[ 0.5608,  0.5765,  0.6000,  ...,  0.9137,  0.9059,  0.9059],\n",
       "          [ 0.5765,  0.5922,  0.6078,  ...,  0.9137,  0.9059,  0.9059],\n",
       "          [ 0.5843,  0.6000,  0.6157,  ...,  0.9137,  0.9137,  0.9059],\n",
       "          ...,\n",
       "          [-0.4039, -0.3804, -0.3333,  ..., -0.2863, -0.3098, -0.2784],\n",
       "          [-0.3961, -0.3725, -0.3647,  ..., -0.2784, -0.2784, -0.2941],\n",
       "          [-0.3804, -0.3882, -0.3882,  ..., -0.2549, -0.3020, -0.3569]],\n",
       "\n",
       "         [[ 0.5373,  0.5608,  0.5765,  ...,  0.9137,  0.9059,  0.9059],\n",
       "          [ 0.5529,  0.5686,  0.5843,  ...,  0.9137,  0.9059,  0.9059],\n",
       "          [ 0.5608,  0.5843,  0.6000,  ...,  0.9137,  0.9137,  0.9059],\n",
       "          ...,\n",
       "          [-0.3882, -0.3647, -0.3176,  ..., -0.3176, -0.3490, -0.3333],\n",
       "          [-0.3725, -0.3490, -0.3412,  ..., -0.3176, -0.3176, -0.3412],\n",
       "          [-0.3569, -0.3647, -0.3647,  ..., -0.3098, -0.3412, -0.3961]]]],\n",
       "       device='cuda:0')}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(input_ids = inputs[\"input_ids\"], pixel_values = inputs[\"pixel_values\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 1024])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['text_model_output']['last_hidden_state'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ColClip were not initialized from the model checkpoint at openai/clip-vit-large-patch14-336 and are newly initialized because the shapes did not match:\n",
      "- visual_projection.weight: found shape torch.Size([768, 1024]) in the checkpoint and torch.Size([128, 1024]) in the model instantiated\n",
      "- text_projection.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([128, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = ColClip.from_pretrained(\"openai/clip-vit-large-patch14-336\", ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ColClip(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 768)\n",
       "      (position_embedding): Embedding(77, 768)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "      (position_embedding): Embedding(577, 1024)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=1024, out_features=128, bias=False)\n",
       "  (text_projection): Linear(in_features=768, out_features=128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1602: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPProcessor\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14-336\")\n",
    "\n",
    "url = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "inputs = processor(text = [\"Hello how are you?\"], images=image, return_tensors=\"pt\", padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'position_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mposition_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:258\u001b[0m, in \u001b[0;36mBatchEncoding.__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;124;03mIf the key is a string, returns the value of the dict associated to `key` ('input_ids', 'attention_mask',\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;124;03metc.).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;124;03mwith the constraint of slice.\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encodings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encodings[item]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'position_ids'"
     ]
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 128])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[\"text\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 576, 128])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[\"vision\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1602: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "Some weights of ColClip were not initialized from the model checkpoint at openai/clip-vit-large-patch14-336 and are newly initialized because the shapes did not match:\n",
      "- visual_projection.weight: found shape torch.Size([768, 1024]) in the checkpoint and torch.Size([128, 1024]) in the model instantiated\n",
      "- text_projection.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([128, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "WARNING:bitsandbytes.cextension:The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pynvml not found. GPU stats will not be printed.\n",
      "Configurating PEFT model\n",
      "trainable params: 8,679,424 || all params: 435,476,737 || trainable%: 1.9931\n"
     ]
    }
   ],
   "source": [
    "import configue\n",
    "config = configue.load(\"colphi3/scripts/configs/clip/train_colclip_model.yaml\", sub_path=\"config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'pixel_values'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForFeatureExtraction(\n",
       "  (base_model): LoraModel(\n",
       "    (model): ColClip(\n",
       "      (text_model): CLIPTextTransformer(\n",
       "        (embeddings): CLIPTextEmbeddings(\n",
       "          (token_embedding): Embedding(49408, 768)\n",
       "          (position_embedding): Embedding(77, 768)\n",
       "        )\n",
       "        (encoder): CLIPEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0-11): 12 x CLIPEncoderLayer(\n",
       "              (self_attn): CLIPSdpaAttention(\n",
       "                (k_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (out_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "              )\n",
       "              (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): CLIPMLP(\n",
       "                (activation_fn): QuickGELUActivation()\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              )\n",
       "              (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (vision_model): CLIPVisionTransformer(\n",
       "        (embeddings): CLIPVisionEmbeddings(\n",
       "          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "          (position_embedding): Embedding(577, 1024)\n",
       "        )\n",
       "        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder): CLIPEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0-23): 24 x CLIPEncoderLayer(\n",
       "              (self_attn): CLIPSdpaAttention(\n",
       "                (k_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=32, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=32, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=32, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (out_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=32, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "              )\n",
       "              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): CLIPMLP(\n",
       "                (activation_fn): QuickGELUActivation()\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (visual_projection): Linear(in_features=1024, out_features=128, bias=False)\n",
       "      (text_projection): lora.Linear(\n",
       "        (base_layer): Linear(in_features=768, out_features=128, bias=False)\n",
       "        (lora_dropout): ModuleDict(\n",
       "          (default): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (lora_A): ModuleDict(\n",
       "          (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "        )\n",
       "        (lora_B): ModuleDict(\n",
       "          (default): Linear(in_features=32, out_features=128, bias=False)\n",
       "        )\n",
       "        (lora_embedding_A): ParameterDict()\n",
       "        (lora_embedding_B): ParameterDict()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[49406,  3306,   829,   631,   592,   286, 49407]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]]), 'pixel_values': tensor([[[[ 1.7114,  1.7114,  1.7114,  ...,  1.8281,  1.8281,  1.8281],\n",
       "          [ 1.7114,  1.7114,  1.7260,  ...,  1.8281,  1.8281,  1.8281],\n",
       "          [ 1.7114,  1.7260,  1.7260,  ...,  1.8281,  1.8281,  1.8281],\n",
       "          ...,\n",
       "          [ 0.0033,  0.1055,  0.1201,  ...,  0.3683,  0.3391,  0.3683],\n",
       "          [-0.0259,  0.0909,  0.0617,  ...,  0.4267,  0.3975,  0.3975],\n",
       "          [-0.0696, -0.0550, -0.0113,  ...,  0.4559,  0.3975,  0.3391]],\n",
       "\n",
       "         [[ 1.8498,  1.8498,  1.8498,  ...,  1.9698,  1.9698,  1.9698],\n",
       "          [ 1.8498,  1.8498,  1.8648,  ...,  1.9698,  1.9698,  1.9698],\n",
       "          [ 1.8498,  1.8648,  1.8648,  ...,  1.9698,  1.9698,  1.9698],\n",
       "          ...,\n",
       "          [ 0.4691,  0.5741,  0.5891,  ...,  0.5891,  0.5741,  0.6041],\n",
       "          [ 0.4240,  0.5591,  0.5441,  ...,  0.6191,  0.5891,  0.6041],\n",
       "          [ 0.3940,  0.3940,  0.4540,  ...,  0.6491,  0.5591,  0.4991]],\n",
       "\n",
       "         [[ 1.9610,  1.9610,  1.9610,  ...,  2.0464,  2.0464,  2.0464],\n",
       "          [ 1.9610,  1.9610,  1.9753,  ...,  2.0464,  2.0464,  2.0464],\n",
       "          [ 1.9610,  1.9753,  1.9753,  ...,  2.0464,  2.0464,  2.0464],\n",
       "          ...,\n",
       "          [ 0.7097,  0.8092,  0.8234,  ...,  0.7808,  0.7523,  0.7808],\n",
       "          [ 0.6812,  0.7808,  0.7808,  ...,  0.8092,  0.7808,  0.7808],\n",
       "          [ 0.6528,  0.7097,  0.7381,  ...,  0.8377,  0.7523,  0.6955]]]])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': tensor([[[ 9.5703e-02, -1.3477e-01,  1.8848e-01, -1.4062e-01, -1.6479e-02,\n",
       "            1.4648e-01, -4.0039e-02,  8.1055e-02, -1.3867e-01, -1.3428e-03,\n",
       "           -5.7129e-02,  3.9551e-02,  2.1484e-02, -5.3955e-02, -1.0205e-01,\n",
       "           -1.2268e-02, -4.8828e-02,  6.2500e-02, -9.8145e-02, -1.2988e-01,\n",
       "           -9.4238e-02, -7.4707e-02, -9.0820e-02, -1.9727e-01, -6.5918e-03,\n",
       "            3.9795e-02,  5.5176e-02,  1.1621e-01,  2.5024e-02,  1.2695e-01,\n",
       "           -3.8330e-02, -8.5449e-02, -3.0396e-02,  1.2158e-01,  4.2480e-02,\n",
       "            9.4238e-02, -3.1738e-02, -7.6172e-02, -1.0840e-01, -3.6865e-02,\n",
       "           -1.8677e-02, -1.8750e-01,  4.6387e-02, -3.6621e-02, -9.7656e-03,\n",
       "            5.3711e-02, -1.5625e-01, -6.2500e-02,  2.8809e-02, -5.0354e-03,\n",
       "            5.4443e-02,  9.0820e-02, -6.2012e-02,  8.1543e-02,  6.8665e-03,\n",
       "           -1.9043e-01, -3.7842e-02,  7.5378e-03,  5.9326e-02,  1.5332e-01,\n",
       "            1.1353e-02,  1.1719e-01, -9.4727e-02, -2.9419e-02, -4.2480e-02,\n",
       "            6.9336e-02, -6.3477e-03,  1.6016e-01, -2.9297e-02, -7.8125e-03,\n",
       "            1.3281e-01,  9.8633e-02, -1.0449e-01,  1.1426e-01,  3.8818e-02,\n",
       "            2.7222e-02, -4.2969e-02,  9.4604e-03, -9.0820e-02, -1.3086e-01,\n",
       "           -2.7344e-02,  1.2695e-01,  2.6123e-02, -5.2002e-02, -9.0332e-02,\n",
       "            1.0498e-01, -6.2500e-02,  2.6001e-02,  9.4727e-02, -4.0283e-02,\n",
       "           -3.5156e-02,  1.9141e-01, -1.4746e-01,  1.0400e-01, -6.4087e-03,\n",
       "           -7.3242e-02,  2.6001e-02,  8.3496e-02,  1.5039e-01, -1.0107e-01,\n",
       "           -3.9795e-02, -2.9053e-02, -2.2583e-02, -1.2305e-01, -1.0352e-01,\n",
       "           -1.7188e-01,  2.6245e-02,  5.7129e-02,  1.3184e-02,  6.5430e-02,\n",
       "            7.6660e-02,  1.6797e-01,  1.1035e-01, -2.6855e-02, -2.2583e-02,\n",
       "            7.4219e-02,  1.0693e-01, -9.6191e-02,  2.3937e-04, -8.3984e-02,\n",
       "           -1.1768e-01,  8.6426e-02, -8.2397e-03, -9.8877e-03, -1.0693e-01,\n",
       "           -8.5938e-02,  5.5420e-02, -1.1182e-01],\n",
       "          [ 1.4941e-01, -6.1340e-03,  1.1084e-01,  2.5391e-02,  1.0596e-01,\n",
       "           -1.6602e-02,  8.2779e-04,  6.0059e-02,  4.9805e-02,  2.7924e-03,\n",
       "            8.2397e-03,  2.7954e-02,  4.9561e-02, -5.0049e-02,  1.1084e-01,\n",
       "            8.2520e-02,  6.5002e-03, -7.1289e-02, -2.0386e-02, -1.4941e-01,\n",
       "           -1.2109e-01, -1.9409e-02,  1.5234e-01, -4.1199e-03,  1.7676e-01,\n",
       "            1.0889e-01, -6.3965e-02, -2.7657e-04, -2.2949e-02,  9.9121e-02,\n",
       "            5.1270e-02, -7.3547e-03, -9.2773e-02,  9.8145e-02,  6.4392e-03,\n",
       "           -9.0820e-02, -3.9062e-02,  4.5166e-02, -8.2031e-02,  8.1543e-02,\n",
       "            5.9814e-02, -1.9531e-01, -9.3262e-02, -5.2979e-02,  3.4668e-02,\n",
       "            1.7871e-01, -1.4526e-02,  1.5234e-01, -1.6724e-02, -1.0498e-02,\n",
       "            1.0498e-01,  7.6172e-02,  1.5381e-02,  8.8867e-02,  1.0596e-01,\n",
       "           -3.0273e-02,  3.8818e-02, -2.4048e-02, -8.0078e-02, -1.4062e-01,\n",
       "            5.9326e-02, -6.3965e-02,  1.9434e-01, -3.2196e-03, -1.3086e-01,\n",
       "           -5.2246e-02, -5.2490e-02,  1.0449e-01,  6.8848e-02, -1.2012e-01,\n",
       "            1.3477e-01, -8.8379e-02, -5.3467e-02,  9.6191e-02,  1.1816e-01,\n",
       "           -7.8125e-02, -1.0645e-01,  1.8555e-02, -8.1055e-02, -1.8555e-02,\n",
       "           -1.9531e-02,  1.0938e-01, -3.5156e-02,  1.5332e-01,  6.1035e-02,\n",
       "            1.5820e-01,  1.6797e-01, -2.2095e-02, -4.5898e-02, -3.3936e-02,\n",
       "           -3.8330e-02,  1.9932e-04, -7.7637e-02,  1.9922e-01, -2.2827e-02,\n",
       "            1.1597e-02,  5.9082e-02,  4.6875e-02,  1.2354e-01,  1.5234e-01,\n",
       "           -4.3945e-02, -2.6245e-02,  4.1992e-02,  2.2339e-02,  1.8359e-01,\n",
       "            1.1523e-01, -1.2817e-02,  7.1289e-02, -9.1797e-02, -1.0938e-01,\n",
       "            1.4954e-02,  6.8848e-02, -1.1301e-04, -9.0332e-02, -8.9355e-02,\n",
       "            6.7383e-02, -3.3691e-02, -1.4648e-01, -1.5332e-01,  1.1279e-01,\n",
       "           -1.0693e-01, -4.7852e-02,  8.8867e-02, -1.3281e-01,  3.8818e-02,\n",
       "           -2.0905e-03,  7.1289e-02,  2.2949e-02],\n",
       "          [-3.4668e-02, -1.6968e-02, -7.6660e-02,  1.6113e-01,  1.7090e-01,\n",
       "           -4.5410e-02,  9.3994e-03,  2.3315e-02, -3.2471e-02, -1.2398e-04,\n",
       "            7.1777e-02, -2.4414e-02, -1.7090e-02, -1.3672e-02,  1.2500e-01,\n",
       "            3.1128e-02, -1.9653e-02,  2.6398e-03, -3.2959e-02, -2.3315e-02,\n",
       "           -1.0498e-01, -1.8555e-02,  2.2266e-01,  6.6406e-02, -1.5747e-02,\n",
       "           -4.5898e-02, -1.7773e-01, -8.0566e-02, -5.7617e-02, -7.0801e-02,\n",
       "            8.0566e-02,  5.5542e-03,  2.3804e-02,  1.5869e-02,  7.7637e-02,\n",
       "            4.1504e-02,  6.2500e-02,  1.2598e-01, -3.3203e-02, -4.4250e-03,\n",
       "            5.9570e-02, -7.8613e-02, -5.6641e-02, -9.8633e-02, -1.6235e-02,\n",
       "            1.2598e-01, -5.0293e-02,  1.1865e-01, -5.6885e-02, -3.4180e-02,\n",
       "            3.9307e-02,  1.4648e-01,  1.6992e-01,  8.3008e-02, -3.2196e-03,\n",
       "            8.1177e-03,  8.3618e-03, -7.2754e-02, -5.3223e-02, -1.2500e-01,\n",
       "           -1.0315e-02, -5.4321e-03,  1.0010e-01, -1.7578e-02, -1.4221e-02,\n",
       "           -2.0605e-01, -9.8145e-02,  3.4424e-02, -7.8125e-02,  6.6376e-04,\n",
       "            1.1914e-01, -1.0352e-01, -9.7046e-03,  1.3477e-01, -1.7090e-02,\n",
       "           -1.0059e-01, -2.0996e-01, -5.3223e-02,  9.1309e-02, -3.2715e-02,\n",
       "           -1.1353e-02,  1.9238e-01, -1.1865e-01,  1.4648e-01,  1.0547e-01,\n",
       "            1.5430e-01,  1.3086e-01,  3.6865e-02, -1.2500e-01,  8.3008e-02,\n",
       "            5.8899e-03, -1.7822e-02,  5.2246e-02,  6.2988e-02, -5.7617e-02,\n",
       "            1.5332e-01,  8.8867e-02, -9.1797e-02, -6.8848e-02,  7.3242e-02,\n",
       "           -9.3750e-02, -8.9355e-02,  1.5625e-02, -7.8613e-02,  1.6602e-01,\n",
       "            1.7090e-02, -1.0693e-01, -1.1475e-02, -1.6016e-01, -6.5430e-02,\n",
       "           -4.2480e-02,  1.3477e-01, -7.6294e-04, -8.2520e-02,  2.9053e-02,\n",
       "           -4.6631e-02, -1.1572e-01,  4.7684e-04, -2.0752e-02,  1.5039e-01,\n",
       "            4.1016e-02, -5.4932e-02,  4.5898e-02, -2.0703e-01,  7.3242e-02,\n",
       "            6.8283e-04,  7.1777e-02,  1.1230e-01],\n",
       "          [ 1.0010e-01, -1.7944e-02, -7.9590e-02,  1.3867e-01,  1.4258e-01,\n",
       "            3.3447e-02,  1.0645e-01, -8.1055e-02,  8.7891e-02,  1.0620e-02,\n",
       "            1.9922e-01, -5.7983e-04, -2.8992e-03, -1.1865e-01,  7.2266e-02,\n",
       "            2.3438e-02, -3.5645e-02, -2.0508e-02, -1.0303e-01,  3.8818e-02,\n",
       "           -5.1270e-02,  4.1748e-02,  5.4932e-02,  2.9419e-02,  1.6699e-01,\n",
       "            1.1719e-02, -1.0205e-01, -3.7842e-02,  2.1240e-02, -4.7913e-03,\n",
       "            1.6211e-01,  5.3711e-02, -8.6060e-03,  9.0408e-04,  2.5146e-02,\n",
       "           -4.3213e-02,  1.4355e-01,  3.6133e-02, -1.0840e-01, -4.8828e-02,\n",
       "           -1.2817e-02,  2.1606e-02, -5.5908e-02, -5.3467e-02, -7.2266e-02,\n",
       "            4.9805e-02,  1.9409e-02,  1.7188e-01, -8.5938e-02, -3.4668e-02,\n",
       "            1.2598e-01,  2.1777e-01,  6.0059e-02,  2.4902e-02,  3.1494e-02,\n",
       "            1.4160e-01,  8.3984e-02, -1.2146e-02, -1.0059e-01, -1.7773e-01,\n",
       "            1.0596e-01,  2.9541e-02,  5.0293e-02, -1.0889e-01,  1.1902e-03,\n",
       "           -2.3047e-01, -8.6426e-02, -2.7466e-03,  6.8848e-02,  6.6406e-02,\n",
       "            1.3184e-01,  6.2561e-03,  1.3477e-01,  8.7891e-02, -7.2266e-02,\n",
       "           -1.2988e-01, -6.6895e-02, -2.2095e-02,  9.3262e-02,  5.5420e-02,\n",
       "           -9.2773e-02,  1.3086e-01, -4.0039e-02,  2.1875e-01,  4.2236e-02,\n",
       "            6.6895e-02,  4.8828e-02,  2.5146e-02, -1.0742e-01,  1.1816e-01,\n",
       "            6.9336e-02, -7.0312e-02,  5.2002e-02, -5.2261e-04, -2.0264e-02,\n",
       "            8.7891e-02,  4.0771e-02, -5.2490e-02, -2.0264e-02,  4.5166e-03,\n",
       "            4.1199e-03, -6.3965e-02,  2.8320e-02, -2.7344e-02,  1.3574e-01,\n",
       "            4.2236e-02, -6.9824e-02,  2.8198e-02, -1.2695e-01, -9.7656e-02,\n",
       "           -1.4954e-02,  8.1543e-02,  6.7383e-02, -3.9551e-02,  3.9795e-02,\n",
       "            1.3504e-03, -8.7891e-02, -1.0400e-01,  4.3457e-02,  2.0410e-01,\n",
       "           -2.6489e-02, -1.7383e-01,  4.3457e-02, -1.7285e-01, -5.8746e-04,\n",
       "            2.6489e-02,  4.4189e-02,  1.4453e-01],\n",
       "          [ 6.4941e-02, -5.7373e-02, -6.6406e-02,  7.7637e-02,  1.3379e-01,\n",
       "            8.6426e-02,  5.2734e-02,  2.4414e-02,  5.4443e-02,  1.3611e-02,\n",
       "            2.0215e-01, -2.7222e-02, -4.3701e-02, -4.2725e-02,  4.1016e-02,\n",
       "            1.2305e-01, -3.1128e-03,  8.8867e-02, -3.6865e-02, -6.2500e-02,\n",
       "           -3.1494e-02, -6.6406e-02,  1.9531e-01, -2.3438e-02,  1.0010e-01,\n",
       "           -1.6846e-02, -1.7285e-01,  1.3123e-03, -8.3984e-02, -3.8574e-02,\n",
       "           -1.1597e-02,  2.5513e-02,  9.7656e-02,  6.8359e-02,  7.3242e-02,\n",
       "            5.8594e-02,  1.1377e-01,  8.5449e-02, -1.4258e-01,  6.2500e-02,\n",
       "           -5.0537e-02, -1.7090e-02, -1.4258e-01, -3.6377e-02, -7.0312e-02,\n",
       "            8.8379e-02,  6.6406e-02,  1.9824e-01,  6.0272e-04, -6.3965e-02,\n",
       "            1.5430e-01,  1.0889e-01,  3.9062e-02,  9.9121e-02,  3.3569e-03,\n",
       "            1.1426e-01, -6.9336e-02,  3.1982e-02, -1.0010e-01, -1.5820e-01,\n",
       "            3.5645e-02,  4.0039e-02,  8.1543e-02, -1.0107e-01, -1.8921e-02,\n",
       "           -1.1279e-01,  4.1260e-02, -4.6143e-02,  1.0156e-01, -8.6914e-02,\n",
       "            6.9824e-02, -3.6865e-02,  2.8687e-02,  6.4941e-02, -1.3184e-01,\n",
       "           -4.5410e-02, -3.4668e-02, -7.4707e-02,  9.1797e-02,  1.3184e-02,\n",
       "           -4.1504e-02,  8.6914e-02, -6.4941e-02,  1.6699e-01,  1.0205e-01,\n",
       "            5.9082e-02,  5.1514e-02, -3.7842e-02, -1.5527e-01, -8.7280e-03,\n",
       "            5.3711e-02,  3.6865e-02,  8.3496e-02, -4.1504e-02, -2.1729e-02,\n",
       "            1.5723e-01, -5.1575e-03, -1.7700e-02,  7.8735e-03,  5.5176e-02,\n",
       "            1.6113e-01,  1.5381e-02,  7.1777e-02,  2.7588e-02,  1.8164e-01,\n",
       "            5.2734e-02, -1.5137e-01, -4.0527e-02, -1.1230e-01,  1.1292e-02,\n",
       "            7.5684e-02,  1.5723e-01,  3.6133e-02, -1.0645e-01, -3.2715e-02,\n",
       "           -8.3984e-02, -1.1035e-01, -8.1543e-02,  4.2480e-02,  2.3242e-01,\n",
       "           -2.3560e-02, -1.1621e-01, -4.8584e-02, -1.3672e-01,  6.7383e-02,\n",
       "            5.0781e-02,  9.2285e-02,  1.2793e-01],\n",
       "          [ 1.0107e-01,  6.9275e-03,  3.2715e-02,  2.4902e-02,  5.2490e-02,\n",
       "            3.8086e-02,  3.6621e-02,  3.3936e-02, -5.6641e-02,  1.9531e-02,\n",
       "            6.6406e-02,  4.1992e-02, -2.6367e-02, -7.9590e-02,  9.7168e-02,\n",
       "            1.6504e-01, -1.2793e-01,  5.7373e-03, -5.7861e-02, -3.0151e-02,\n",
       "           -6.1768e-02, -1.6602e-02,  1.5039e-01, -4.7852e-02,  1.5723e-01,\n",
       "           -6.9336e-02, -2.4512e-01, -1.2756e-02, -6.7871e-02, -1.0938e-01,\n",
       "            1.1914e-01,  3.7842e-03,  1.2756e-02,  4.9561e-02,  1.1377e-01,\n",
       "            3.9551e-02,  1.3770e-01,  9.8145e-02, -6.1279e-02,  3.5645e-02,\n",
       "            6.0730e-03, -1.7090e-01, -1.3184e-01, -7.5195e-02,  2.4536e-02,\n",
       "            2.0410e-01, -1.8433e-02,  1.6309e-01,  1.6174e-03, -1.0107e-01,\n",
       "            9.4238e-02,  6.2256e-02,  4.1748e-02,  7.8125e-02, -2.5757e-02,\n",
       "           -2.3956e-03, -4.7852e-02,  5.6641e-02, -6.2256e-02, -7.6660e-02,\n",
       "            2.6855e-02,  1.0010e-01,  9.6191e-02, -8.8379e-02, -1.2305e-01,\n",
       "           -4.0771e-02,  3.7842e-02,  1.8066e-02,  4.8828e-02, -2.4719e-03,\n",
       "            1.0596e-01, -1.1963e-01,  4.1504e-02, -2.0508e-02, -2.2507e-04,\n",
       "           -1.2878e-02, -1.6406e-01, -4.1992e-02,  4.5538e-05,  1.3574e-01,\n",
       "           -2.5269e-02,  1.1377e-01, -3.6621e-02,  7.1777e-02,  6.0303e-02,\n",
       "            5.0537e-02,  1.3965e-01,  5.0537e-02, -2.1484e-01,  1.0889e-01,\n",
       "            1.1523e-01,  7.7148e-02, -1.2598e-01,  4.9561e-02, -4.5654e-02,\n",
       "            3.7354e-02, -5.8899e-03, -1.7578e-02,  4.8096e-02,  4.2236e-02,\n",
       "            6.6895e-02,  4.6692e-03,  2.6367e-02, -3.9978e-03,  1.5625e-01,\n",
       "            2.3804e-02, -9.4238e-02, -6.5918e-02, -1.7578e-01, -4.8584e-02,\n",
       "            7.8125e-02,  1.5039e-01,  6.4453e-02, -1.5723e-01, -5.2490e-02,\n",
       "            4.2236e-02, -5.7861e-02, -1.1279e-01, -2.1606e-02,  1.3184e-01,\n",
       "           -6.5918e-02, -4.3213e-02,  8.4473e-02, -1.8359e-01,  1.4746e-01,\n",
       "           -1.0986e-03,  6.4453e-02,  1.1328e-01],\n",
       "          [ 1.8555e-01, -4.3701e-02, -3.4424e-02, -1.6357e-02,  8.3008e-02,\n",
       "            3.6377e-02,  6.8665e-03, -7.3853e-03, -1.2061e-01,  4.1504e-02,\n",
       "            1.0986e-01,  3.5400e-02, -3.6377e-02,  3.1128e-02,  4.4922e-02,\n",
       "            1.9531e-01, -1.2158e-01,  4.8584e-02, -5.1270e-02, -2.7832e-02,\n",
       "           -6.3965e-02, -8.2520e-02,  1.4746e-01, -3.2471e-02,  1.8262e-01,\n",
       "           -3.0884e-02, -2.4707e-01,  9.1309e-02,  2.9541e-02, -6.8848e-02,\n",
       "            2.3682e-02,  1.0452e-03,  8.8867e-02,  6.9336e-02,  9.7656e-02,\n",
       "            9.5215e-02,  3.1982e-02,  3.3203e-02, -1.2598e-01, -3.9551e-02,\n",
       "            2.8320e-02, -1.7480e-01, -3.2227e-02, -1.1572e-01,  5.0537e-02,\n",
       "            2.0020e-01, -7.1289e-02,  5.8105e-02, -2.3438e-02, -4.4922e-02,\n",
       "            1.7090e-01,  5.2002e-02,  2.2949e-02,  1.1523e-01, -4.0771e-02,\n",
       "           -4.6875e-02, -1.3489e-02,  3.7109e-02,  3.5645e-02, -7.1777e-02,\n",
       "           -4.5654e-02,  1.5625e-01,  9.1553e-04, -1.0791e-01, -6.0547e-02,\n",
       "           -1.2695e-01,  3.9307e-02,  5.0537e-02, -7.7148e-02,  7.1777e-02,\n",
       "            5.2002e-02, -6.4453e-02, -2.0386e-02,  2.8564e-02, -8.7891e-02,\n",
       "            9.0790e-04, -1.2451e-01, -8.6426e-02, -4.1260e-02,  8.5449e-02,\n",
       "            4.7852e-02,  6.1768e-02,  1.9165e-02,  6.8359e-02,  8.1543e-02,\n",
       "            2.9297e-02,  5.1270e-02,  6.5918e-02, -1.6602e-01,  1.5137e-01,\n",
       "            5.4443e-02,  1.2793e-01, -1.8359e-01,  1.5991e-02, -3.9795e-02,\n",
       "            1.5918e-01,  5.5908e-02, -4.5654e-02,  1.3379e-01,  7.5989e-03,\n",
       "            1.1621e-01,  1.2500e-01,  9.1797e-02,  1.5564e-02,  4.1748e-02,\n",
       "           -9.6680e-02, -1.4844e-01, -1.8066e-02, -3.1494e-02, -2.5024e-02,\n",
       "           -3.6133e-02,  1.3477e-01, -2.5146e-02, -1.0400e-01, -9.3750e-02,\n",
       "            2.3438e-02,  2.8076e-02, -8.2520e-02,  2.2217e-02,  1.1035e-01,\n",
       "           -6.0791e-02, -9.1797e-02,  1.8555e-02, -1.8750e-01,  1.4453e-01,\n",
       "            4.4678e-02,  5.3223e-02, -2.9541e-02]]], dtype=torch.bfloat16,\n",
       "        grad_fn=<MulBackward0>),\n",
       " 'vision': tensor([[[-0.0347, -0.0172,  0.0386,  ..., -0.2051,  0.0253, -0.0280],\n",
       "          [-0.1201,  0.0359,  0.1416,  ..., -0.1299, -0.0271, -0.0229],\n",
       "          [-0.1099,  0.1177,  0.0864,  ..., -0.1572, -0.0049, -0.0781],\n",
       "          ...,\n",
       "          [-0.1309,  0.0708,  0.0679,  ..., -0.0126, -0.0618, -0.0244],\n",
       "          [-0.0679,  0.0610, -0.0072,  ...,  0.0219, -0.0294, -0.0806],\n",
       "          [-0.1221, -0.0393,  0.0217,  ..., -0.1807, -0.0508, -0.0425]]],\n",
       "        dtype=torch.bfloat16, grad_fn=<DivBackward0>)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.model(input_ids = inputs[\"input_ids\"], attention_mask = inputs[\"attention_mask\"], pixel_values = inputs[\"pixel_values\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
